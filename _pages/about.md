---
layout: about
title: about
permalink: /
subtitle: 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    Email: mhrezaei@arizona.edu

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a final-year undergraduate student at the [**University of Arizona**](https://www.arizona.edu/) <img src="assets/img/uofa.png" alt="UArizona" height="20px"/> majoring in [**Computer Science**](https://www.cs.arizona.edu/) with a 4.0 GPA. I am a member of the [**Computational Language Understanding (CLU) Lab**](https://clulab.org/) <img src="assets/img/clulab.png" alt="CLU lab" height="20px"/> advised by [Dr. Eduardo Blanco](https://eduardoblanco.github.io/), where I worked on [making language models robust against negation](https://aclanthology.org/2025.naacl-long.413), [improving negation understanding by paraphrasing in affirmative terms](https://aclanthology.org/2024.acl-short.55/), and [interpreting indirect question answering in multiple languages](https://aclanthology.org/2023.findings-emnlp.146/).

Previously, I was a research intern at [**Stanford University**](https://www.stanford.edu/) <img src="assets/img/stanford.png" alt="Stanford" height="20px"/> in the [**SALT Lab**](https://saltlab.stanford.edu/) <img src="assets/img/salt_logo.svg" alt="SALT" height="20px"/> advised by [Dr. Diyi Yang](https://cs.stanford.edu/~diyiy/). There, I co-created [**EgoNormia**](https://egonormia.org), a benchmark for evaluating physical-social norm understanding in vision-language models.

In summer 2025, I was a post-training research intern at [**Scale AI**](https://scale.com/) <img src="assets/img/scale_logo.jpg" alt="Scale AI" height="20px"/>, where I worked on [*Online Rubrics Elicitation from Pairwise Comparisons*](https://arxiv.org/abs/2510.07284), an approach for post-training LLMs with evolving rubrics to improve alignment in tasks without verifiable ground-truth.  